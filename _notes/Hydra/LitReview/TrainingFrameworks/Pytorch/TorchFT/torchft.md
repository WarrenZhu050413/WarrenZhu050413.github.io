Full Run Through
从Optimizer Wrapper就可以看出来，Manager的function都在 start_quorum()和should_commit() 里面。每一个forward pass之前都需要start_quorum(), and after each backward pass, we'll need to check whether we should_commit the computed gradients to optimization.Optimizer Wrapper
def zero_grad(self, set_to_none: bool = True) -> None:
    self.manager.start_quorum()
    self.optim.zero_grad(set_to_none)
def step(self, closure: Optional[object] = None) -> None:
    if self.manager.should_commit():
        self.optim.step()

GRPC Setup
比较复杂的是他有两层grpc call，从manager client到manager server，manager server到lighthouse server。每次quorum会先manager client quorum到manager server，manager server 收到足够的quorum 就会quorum lighthouse server。
这个Manager这一层我们是需要手动启动的，指定哪一些machine在一个manager下面（一个replica group）给每一个machine一个rank，这个可以用torch device mesh或者自己specify process group。我们也需要手动启动一个lighthouse server
LighthouseService set up through _run_grpc whenever a python LighthouseServer object is created.
lighthouse = LighthouseServer(
    bind="[::]:0",
    min_replicas=1,
    join_timeout_ms=100,
)

ManagerService set up whenever the python Manager object is created for a machine of rank 0. 
ManagerClient set up whenever the python Manager object is created, and it connects via the Tonic autogenerated method ManagerServiceClient::new(conn) to have a client connected to the ManagerServer. 
self.manager.start_quorum()
- 这个会做所有quorum方面的准备。全通过grpc。
- Setup ManagerClient and ManagerServer, 这个会直接spawn thread来做heartbeat。
- 还会做live checkpoint recovery
'start_quorum'
When calling start_quorum before each forward pass (e.g. in the OptimizerWrapper.zero_grad()):
From the calling Manager
Manager.start_quorum 
-> Manager._async_quorum 
-> ManagerClient._quorum 
-> (ManagerClient gRPC call) ManagerService.quorum
ManagerServer
ManagerService.quorum (ManagerServer, state updates) 
-> Manager._run_quorum
-> (if (state.participants.len() as u64) < self.world_size, invokes gRPC call) LighthouseService.quorum
LighthouseServer
-> Lighthouse._run_quorum
-> Lighthouse.quorum_tick (ticks every interval(Duration::from_millis(self.opt.quorum_tick_ms));, LighthouseServer) 
-> Lighthouse.quorum_compute (checks whether current quorum is valid, condition listed out in the document in a table format, LighthouseServer) 
-> returns (quorum_met, reason) [(Option<Vec<QuorumMember>>, String)] to ManagerServer
-> ManagerServer compute_quorum_results for every single ManagerClient based on their rank and the Manager's replica_id. Note that compute_quorum_results is a pure function.
fn compute_quorum_results(
    replica_id: &str,
    rank: i64,
    quorum: &Quorum,
) -> Result<ManagerQuorumResponse, Status>
Returns the following, which includes both healing information as well as storage information, to the Manager._async_quorum call.
    Ok(ManagerQuorumResponse {
        quorum_id: quorum.quorum_id,
        // address is used for looking up the checkpoint server address.
        recover_src_manager_address: recover_src_manager_address,
        recover_src_rank: recover_src_rank,
        recover_dst_ranks: recovery_assignments
            .get(&replica_rank)
            .map_or_else(Vec::new, |v| v.clone()),
        store_address: primary.store_address.clone(),
        max_step: max_step,
        max_rank: max_rank,
        max_world_size: max_participants.len() as i64,
        replica_rank: replica_rank as i64,
        replica_world_size: participants.len() as i64,
        heal: heal,
    })
-> _async_quorum uses it to configure Manager information as well as manages healing (Live Checkpoint Recovery)
Note: The message delivery between the different servers and clients is through a channel mechanism. Where a client subscribes to the server's self.state.channel whenever it requests quorum. When the server finishes calculating a quorum it broadcasts to all the subscribed client
Recovery
Recovery的逻辑在 _async_quorum (manager.py) 还有compute_quorum_results (manager.rs) 里面。
_async_quorum is called when we call sanager.start_quorum 
在_async_quorom 里面会按照compute_quorum_results 算出来的 recover_dst_ranks and recover_src_rank 来做 checkpoint transport (用的是http/pg transport logic, see Danny's doc). 
Have two control variable: allow_heal and heal. Allow_heal is user-defined, set when launching start_quorum. Heal is set by the ManagerServer in the quorum that it passes down to the server. 
Have a state variable: self._healing. Used to indicate that the current replica is in the process of healing. If healing, then not participating in the current quorum.
if allow_heal:
    ...
    self._checkpoint_transport.send_checkpoint(
            dst_ranks=quorum.recover_dst_ranks,
            step=max_step,
            state_dict=self._manager_state_dict(),
            timeout=self._timeout,
        )
        
if heal:
    self._pending_state_dict = self._checkpoint_transport.recv_checkpoint(
        src_rank=recover_src_rank,
        metadata=checkpoint_metadata,
        step=max_step,
        timeout=self._timeout,
    )
If quorum is not async (in start_quorum()), then does eager recovery: 
###  start_quorum() in manager.py
if not self._use_async_quorum:
    self.wait_quorum()

    if self._healing:
        ###  eagerly apply pending state_dict so we can run the forwards pass
        self._apply_pending_state_dict()

        ###  we are forcing healing at the beginning so we're in a good state
        ###  and don't need to zero_grad
        self._healing = False

How synchronization works: 
start_quorum is applied before the forward pass.
Whenever the quorum finishes (in compute_quorum. 
This starts the live checkpoint recovery process.
During the forward pass, after the quorum is computed, the checkpoint is transported, and then loaded.
When calling should_commit after the backward pass and before the optimizer step, apply the state_dict if the rank is healing. 

should_commit()
self.num_participants()
-> self._wait_quorum()
-> When quorum finishes, sets `self._participating_world_size and self._partcipating_rank
(FIXED_WITH_SPARES means that no extra machine should join beyond the minimum)
self._participating_rank, self._participating_world_size = (
    (max_rank, max_world_size)
    if self._use_async_quorum or not allow_heal
    else (replica_rank, replica_world_size)
)
...
if self._world_size_mode == WorldSizeMode.FIXED_WITH_SPARES:
    self._participating_world_size = min(
        self._participating_world_size, self._min_replica_size
    )
->self._participating_world_size
High Level Architecture
[Image]
1. High Level
TorchFT有三层
a) Lighthouse
b) Manager 
c) Individual Machines
Lighthouse and Manager都是用grpc。ManagerServer同时是LighthouseClient。他们主要的function是run quorum以及
Components
Python:
checkpointing/*
coordination.py: low-level APIs (LighthouseServer, ManagerClient, ManagerServer)
data.py: DistributedSampler
ddp.py
fsdp.py
future.py
http.py
local_sgd.py
manager.py
multiprocessing.py
optim.py
parameter_server.py
process_group.py
torchx.py

Rust:
lib.rs
lighthouse.rs
manager.rs
net.rs
retry.rs
timeout.rs
Both Lighthouse and Manager are implemented with a Rust backend. Manager also has a python frontend. .run() is called in the LighthouseServer and LighthouseManager constructor. 
Lighthouse
Keep a list of ManagerServer (QuorumMember) and their heartbeat information. (See the Manager section).
Its internal heartbeat and participants state is updated whenever a LighthouseClient invokes the LighthouseService.quorum gRPC call. This is then checked by _run_quorum() (see below) to see whether a quorum can be formed. 
struct State {
    channel: broadcast::Sender<Quorum>,
    participants: HashMap<String, QuorumMemberDetails>,
    prev_quorum: Option<Quorum>,
    quorum_id: i64,
    heartbeats: HashMap<String, Instant>,
}
In the background, runs two threads: 
set.spawn(self.clone()._run_quorum());
set.spawn(self.clone()._run_grpc());
Lighthouse._run_quorum()
- For every quorum_tick, check whether quorum_met. If so, then broadcasts the quorum configuration to all the lighthouse clients.
- quorum_met if quorum_compute decides that we have a valid quorum. See the conditions that it checks in this table. 
- Determines healthy replicas through state.heartbeats (if last heartbeat is less than heartbeat_timeout_ms ago). 
LighthouseService.quorum()
- Keep on looping until the replica_id of the calling LighthouseClient(i.e. ManagerServer) is included in a quorum, and then returns the quorum. 
    async fn quorum(
        &self,
        request: Request<LighthouseQuorumRequest>,
    ) -> Result<Response<LighthouseQuorumResponse>, Status> {
        let req = request.into_inner();
        let requester = req
            .requester
            .ok_or_else(|| return Status::invalid_argument("missing requester"))?;

        info!(
            "Received quorum request for replica {}",
            &requester.replica_id
        );

        let mut rx = {
            let mut state = self.state.lock().await;

            // implicit heartbeat
            state
                .heartbeats
                .insert(requester.replica_id.clone(), Instant::now());

            state.participants.insert(
                requester.replica_id.clone(),
                QuorumMemberDetails {
                    joined: Instant::now(),
                    member: requester.clone(),
                },
            );
            let rx = state.channel.subscribe();

            // proactively run quorum tick
            self.clone()
                ._quorum_tick(&mut state)
                .map_err(|e| Status::from_error(e.into()))?;

            rx
        };

        let quorum = loop {
            let current_quorum = rx.recv().await.map_err(|e| Status::from_error(e.into()))?;

            if current_quorum
                .participants
                .iter()
                .any(|p| p.replica_id == requester.replica_id)
            {
                break current_quorum;
            }

            // Only continue the loop if the replica is not in the quorum
            let mut state = self.state.lock().await;
            state.participants.insert(
                requester.replica_id.clone(),
                QuorumMemberDetails {
                    joined: Instant::now(),
                    member: requester.clone(),
                },
            );
            info!("Replica {} not in quorum, retrying", &requester.replica_id);
        };

        let reply = LighthouseQuorumResponse {
            quorum: Some(quorum),
        };

        Ok(Response::new(reply))
    }

(Lighthouse) quorum_compute()
This content is only supported in a Feishu Docs
可提升方面
- Fast Quorum means that cannot incorporate new replica groups (problematic?)
_run_grpc()
- 开启LighthouseServer
- Lighthouse client initialization在ManagerServer的logic里面
- Receives quorum requests and heartbeats
Note
Heartbeat can be embedded in quorum requests

Manager
Manager has two sets of functions:
- Manages quorum (similar to lighthouse)
- Coordinates live checkpoint recovery
It has a Python Frontend in manager.py and a Rust Backend in manager.rs. lib.rs defines the gRPC interface. The rank 0 machine initializes a ManagerServer, whilst the ManagerClient is initialized at every rank.
```
从Optimizer Wrapper就可以看出来，Manager的function都在 start_quorum()和should_commit() 里面。每一个forward pass之前都需要start_quorum(), and after each backward pass, we'll need to check whether we should_committhe computed gradients to optimization.
Manager States
- quorum_id is 
- replica_id is made unique with a uuid.uuid4() generation
self._step = 0
self._quorum_id = -1
self._errored: Optional[Exception] = None
self._healing = False
self._pending_work: List[torch.futures.Future[object]] = []
self._batches_committed = 0

# first step is 1
self._participating_rank: Optional[int] = None
self._participating_world_size: int = 0
ManagerServer Init
// https://github.com/pytorch/torchft/blob/main/src/manager.rs#L55
struct ManagerState {
    checkpoint_metadata: HashMap<i64, String>,
    channel: broadcast::Sender<Quorum>,
    participants: HashSet<i64>,

    should_commit_channel: broadcast::Sender<bool>,
    should_commit_failures: HashSet<i64>,
    should_commit_count: HashSet<i64>,
}
ManagerServer calls .run() in the Rust Backend to spawn two threads to_run_heartbeat() and _run_grpc().
- run_heartbeat()sends heartbeat to the LighthouseServer every self.heartbeat_interval. 
- _run_grpc() sets up the ManagerServer.
Optimizer Wrapper (How to use Manager)
- Start a manager
- For all the machines that we want to be governed by a manager, we do optimizer = OptimizerWrapper(manager, optimizer)
- "zero_grad() must be called at the start of the forwards pass" to call self.manager.start_quorum().
- step() must be called at the end of the backwards pass.
def zero_grad(self, set_to_none: bool = True) -> None:
    self.manager.start_quorum()
    self.optim.zero_grad(set_to_none)
def step(self, closure: Optional[object] = None) -> None:
    if self.manager.should_commit():
        self.optim.step()
Limitations
- Does not support optimizers that use "closure" (e.g. LBFGS)
start_quorum()
ManagerClient: start_quorum -> _async_quorum -> self.client._quorum -> quorum (gRPC call to ManagerServer)

quorum (ManagerServer, updates state) -> (when meets replica_group conditions) quorum (gRPC call to LighthouseServer) -> returns quorum object
In the Python Frontend manager.py, start_quorum calls _async_quorum calls self.client._quorum, which calls quorum in the Rust Backend manager.rs.
The async_quorum is called through self.quorum_future = elf._executor.submit(self._async_quorum) for asynchronous execution. If don't want the quorum to be async, then waits on the quorum through quocum_wait(), which waits for the result of the quorum_future. 
async_quorum uses the ManagerClient on the machine to communicates to the ManagerServer that the machine is ready.
ManagerServer then communites with LighthouseServer when enough machines in the ManagerServer's replica group are ready through another quorum gRPC call. 
When received enough quorum requests, or after timeout, LighthouseServerwill return a quorum object that includes both the live recovery information as well as the quorum information. 
def start_quorum(
    self,
    allow_heal: bool = True,
    shrink_only: bool = False,
    timeout: Optional[timedelta] = None,
) -> None:
    # wait for previous quorum to complete
    if self._quorum_future is not None:
        self._quorum_future.result()

    self._errored = None
    self._healing = False

    self._quorum_future = self._executor.submit(
        self._async_quorum,
        allow_heal=allow_heal,
        shrink_only=shrink_only,
        quorum_timeout=timeout or self._quorum_timeout,
        curr_device=(
            torch.cuda.current_device() if torch.cuda.is_available() else -1
        ),
    )
    if not self._use_async_quorum:
        self.wait_quorum()

        if self._healing:
            # eagerly apply pending state_dict so we can run the forwards pass
            self._apply_pending_state_dict()

            # we are forcing healing at the beginning so we're in a good state
            # and don't need to zero_grad
            self._healing = False
should_commit()
Two conditions for local_should_commit = enough_replicas and self._errored is None
Increments the steps and the batches commited

def should_commit(self, timeout: Optional[timedelta] = None) -> bool:
    for work in self._pending_work:
        # check at the beginning of since .wait() may trigger errors
        if self._errored is not None:
            break
        work.wait()

    self._pending_work = []

    # apply state_dict if healing
    if self._healing:
        self._apply_pending_state_dict()

    enough_replicas = self.num_participants() >= self._min_replica_size
    local_should_commit = enough_replicas and self._errored is None
    should_commit = self._client.should_commit(
        self._rank,
        self._step,
        local_should_commit,
        timeout=timeout or self._timeout,
    )
    self._logger.info(
        f"should_commit={should_commit} enough_replicas={enough_replicas}, errored={self._errored}"
    )

    self._checkpoint_transport.disallow_checkpoint()

    # decide whether we're in a healthy state to increase the step count
    if should_commit:
        self._step += 1
        self._batches_committed += self.num_participants()

    return should_commit
Healing
In should_commit
if self._healing:
    self._apply_pending_state_dict()
Python Frontend
start_quorum
Calls self._client.quorum. See ManagerClient section. 
Computes a new quorum (potentially asynchronously) and readies the manager for a new step.

It's best practice to call this before the forwards pass of each step for performance as computing quorum may take some time.
quorum = self._client._quorum(
    rank=self._rank,
    step=self._step,
    checkpoint_metadata=self._checkpoint_transport.metadata(),
    shrink_only=shrink_only,
    timeout=quorum_timeout,
)
ManagerClient
ManagerClient exposes three functions
- _quorum invokes the gRPC quorum method on the remote ManagerServer
- ManagerQuorumRequest object and a timeout duration metadata is passed in the method call 
- For more detail on when quorum is called, see the start_quorum() section
// https://github.com/pytorch/torchft/blob/main/src/lib.rs#L152
let mut request = tonic::Request::new(ManagerQuorumRequest {
    rank: rank,
    step: step,
    checkpoint_metadata: checkpoint_metadata,
    shrink_only: shrink_only,
});

request.set_timeout(timeout);
...

let response = self.runtime.block_on(self.client.clone().quorum(request))?;
and constructs a QuorumResult object from the response:
Ok(QuorumResult {
    quorum_id: resp.quorum_id,
    replica_rank: resp.replica_rank,
    replica_world_size: resp.replica_world_size,
    recover_src_manager_address: resp.recover_src_manager_address,
    recover_src_rank: resp.recover_src_rank,
    recover_dst_ranks: resp.recover_dst_ranks,
    store_address: resp.store_address,
    max_step: resp.max_step,
    max_rank: resp.max_rank,
    max_world_size: resp.max_world_size,
    heal: resp.heal,
})
ManagerService.quorum
quorum is implemented in the Rust Backend. 
It gives the information of the machine running the ManagerClient that invokes the gRPC quorum method to the ManagerServer on rank0 through self._run_quorum 
Then ManagerServer saves in its state the participant's checkpoint_metadata and rank. 
When enough ManagerClients have called (state.participants.len() as u64) < self.world_size (the ManagerServer would invoke the gRPC method LighthouseServer.quorum with the aggregated information of the participants to the LighthouseServer in _run_quorum through client.quorum(lighthouse_request). 
LighthouseServer responds with a quorum object.
Then compute_quorum_results is called to see whether the current Manager Replica is in the quorum. 
See below:
// 这个是我加过comments之后的quorum function
#[tonic::async_trait]
impl ManagerService for Arc<Manager> {
    async fn quorum(
        &self,
        request: Request<ManagerQuorumRequest>,
    ) -> Result<Response<ManagerQuorumResponse>, Status> {
        // Parse metadata
        let req = request.get_ref();
        let rank = req.rank;
        ...
        // Set grpc timeout from metadata
        let timeout = try_parse_grpc_timeout(&request.metadata())
            .map_err(|e| {
                Status::invalid_argument(format!(
                    "invalid timeout {}",
                    e.to_str().unwrap_or("invalid")
                ))
            })?
            .ok_or_else(|| Status::invalid_argument("missing timeout"))?;
        
        // Setting up channel to receive quorum from Lighthouse
        let mut rx = {
            let mut state = self.state.lock().await;

            // Store checkpoint_metadata for recovery
            state
                .checkpoint_metadata
                .insert(req.rank, req.checkpoint_metadata.clone());
            ...
            // Store participant information into state
            state.participants.insert(rank);
            let rx = state.channel.subscribe();
            
            // Sends quorum request to LighthouseServer if length of state.participants is greater than self.world_size
            // Broadcasts response to the quorum through state.channel.send(resp.quorum)
            // Always returns OK(())
            self._run_quorum(
                &mut state,
                QuorumMember {
                    replica_id: self.replica_id.clone(),
                    address: self.address(),
                    store_address: self.store_address.clone(),
                    step: req.step,
                    world_size: self.world_size,
                    shrink_only: req.shrink_only,
                },
                timeout,
            )
            .await?;

            rx
        };
        
        // Gets the quorum response from the lighthouse
        let quorum = rx
            .recv()
            .await
            .map_err(|e| Status::internal(e.to_string()))?;
        ...
        let reply = compute_quorum_results(&self.replica_id, rank, &quorum)?;

        Ok(Response::new(reply))
    }

It calls self._run_quorum self.compute_quorum_results, which computes recovery assignments (see Recovery
ManagerService.compute_quorum_results
- Compute replica rank

fn compute_quorum_results(
    replica_id: &str,
    rank: i64,
    quorum: &Quorum,
) -> Result<ManagerQuorumResponse, Status> {
    let mut participants = quorum.participants.clone();
    participants.sort_by(|a, b| a.replica_id.cmp(&b.replica_id));

    // Compute the rank of the replica in the returned quorum.
    let replica_rank = participants
        .iter()
        .enumerate()
        .find_map(|(i, p)| {
            if p.replica_id == replica_id {
                Some(i)
            } else {
                None
            }
        })
        .ok_or_else(|| {
            Status::not_found(format!(
                "replica {} not participating in returned quorum",
                replica_id
            ))
        })?;

    let step = participants[replica_rank].step;

    // Compute the details for workers at max step.
    let max_step = participants.iter().map(|p| p.step).max().unwrap();
    let max_participants: Vec<&QuorumMember> =
        participants.iter().filter(|p| p.step == max_step).collect();
    let max_rank = max_participants.iter().enumerate().find_map(|(i, p)| {
        if p.replica_id == replica_id {
            Some(i as i64)
        } else {
            None
        }
    });

    // The primary TCPStore to use for this rank.
    let primary_rank = rank as usize % max_participants.len();
    let primary = max_participants[primary_rank];

    // Compute recovery assignments

    // Nodes are recovering if:
    // 1. not at the max step
    // 2. max_step == 0 and not the primary replica
    let all_recover_dst_ranks: Vec<usize> = participants
        .iter()
        .enumerate()
        .filter_map(|(i, p)| {
            if p.step != max_step || max_step == 0 && primary.replica_id != p.replica_id {
                Some(i)
            } else {
                None
            }
        })
        .collect();
    let all_recover_dst_ranks_set = all_recover_dst_ranks.iter().collect::<HashSet<_>>();
    let up_to_date_ranks: Vec<usize> = participants
        .iter()
        .enumerate()
        .filter_map(|(i, _p)| {
            if !all_recover_dst_ranks_set.contains(&i) {
                Some(i)
            } else {
                None
            }
        })
        .collect();

    // This is a map of rank to the ranks that are recovering from that node.
    let mut recovery_assignments: HashMap<usize, Vec<i64>> = HashMap::new();
    // The rank of the node that this rank is recovering from.
    let mut recover_src_rank: Option<i64> = None;
    for (i, recovering_rank) in all_recover_dst_ranks.iter().enumerate() {
        let up_to_date_idx = (i + rank as usize) % up_to_date_ranks.len();
        let recovering_recover_src_rank = up_to_date_ranks[up_to_date_idx];
        if !recovery_assignments.contains_key(&recovering_recover_src_rank) {
            recovery_assignments.insert(recovering_recover_src_rank, Vec::new());
        }
        recovery_assignments
            .get_mut(&recovering_recover_src_rank)
            .unwrap()
            .push(*recovering_rank as i64);
        if *recovering_rank == replica_rank {
            recover_src_rank = Some(recovering_recover_src_rank as i64);
        }
    }

    let heal = recover_src_rank.is_some();
    if heal {
        info_with_replica!(
            replica_id,
            "healing is required step={}, max_step={}, recover_src_rank={}",
            step,
            max_step,
            recover_src_rank.unwrap()
        );
    }

    let recover_src_manager_address = match recover_src_rank {
        Some(r) => participants[r as usize].address.clone(),
        None => "".to_string(),
    };

    Ok(ManagerQuorumResponse {
        quorum_id: quorum.quorum_id,
        // address is used for looking up the checkpoint server address.
        recover_src_manager_address: recover_src_manager_address,
        recover_src_rank: recover_src_rank,
        recover_dst_ranks: recovery_assignments
            .get(&replica_rank)
            .map_or_else(Vec::new, |v| v.clone()),
        store_address: primary.store_address.clone(),
        max_step: max_step,
        max_rank: max_rank,
        max_world_size: max_participants.len() as i64,
        replica_rank: replica_rank as i64,
        replica_world_size: participants.len() as i64,
        heal: heal,
    })
}
To Improve: 
- Why give all the quorum information? This just increases the wire transfer side and we don't need it? Or because using broadcast channel?
Checkpointing
Each manager has a statedict. it has the optimization step that the manager is at, along with how many batches it has commited (batches_commited). This ensures that we can checkpoint to the correct state.
def state_dict(self) -> Dict[str, int]:
    """
    Get the state dict for this manager.

    This can be used to checkpoint the state of the manager to restore
    from a previous checkpoint.

    Returns:
        the state dict for this manager
    """
    return {"step": self._step, "batches_committed": self._batches_committed}
Fault Tolerant Process Groups

Created through self._pg.configure(store_prefixed_addr, replica_rank, replica_world_size)in Manager.py. 
在ProcessGroup上面构建的. 有一个ManagedProcessGroup的abstraction。ProcessGroup是可以用
For other libraries we can use an error tolerant version of a ProcessGroup. This extends the reconfigurable PG to also swallow all errors rather than expose them to the modeling code. This allows us to integrate cleanly with libraries such as FSDP with minimal changes required.

By swallowing all errors and always reporting a success to the modeling code in the simplest case requires no code changes to the underlying libraries and we can decide whether or not to commit the optimizer change at the end of the training loop.

Required library contract:
When using dynamic batch sizes, libraries need to be tolerant to changing world sizes of the ProcessGroups and check PG.world_size in the comm hooks and rescale accordingly.
Libraries need to be tolerant to "default/zero" value tensors that are returned by the ProcessGroup on error as the true value is unknown.
Libraries must have a fully deterministic communication pattern that isn't stateful. I.e. no broadcasts on first batch and have identical collective calls on each step of the model
- Graceful handling of communication error. Never explicitly returns error, but a dummy result, and stores error in 
  - Manager informed of the error.
  - Uses ErrorSwallowingProcessGroupWrapper 
  - _ManagedWork overrides 1wait()` and get_future() to add manager.report_error(e). 
  - By doing so, any communication failure is caught and relayed to the Manager, rather than halting the program. These mechanisms are unique to TorchFT’s architecture, as stock PyTorch DDP would typically either block or abort on such errors.
DataSampler
https://github.com/pytorch/pytorch/blob/main/torch/utils/data/distributed.py 
TorchFT用的是Torch.Distributed里的 DistributedSampler, superclass of Sampler.
- 根据global_rank and global_world_size来分配data indices (across all managers)
<torchft/torchft/data.py>
class DistributedSampler(data.distributed.DistributedSampler):
    def __init__(
        self,
        dataset: data.Dataset,
        replica_group: int,
        num_replica_groups: int,
        rank: Optional[int] = None,
        num_replicas: Optional[int] = None,
        **kwargs: object,
    ) -> None:
        self.global_rank: int = rank + num_replicas * replica_group
        self.global_world_size: int = num_replicas * num_replica_groups
        (...)
        super().__init__(
            dataset,
            rank=self.global_rank,
            num_replicas=self.global_world_size,
            # pyre-fixme[6]: got object
            **kwargs,
        )

<pytorch/torch/utils/data/distributed.py>
class DistributedSampler(Sampler[_T_co]):
    def __init__(
        self,
        dataset: Dataset,
        num_replicas: Optional[int] = None,
        rank: Optional[int] = None,
        shuffle: bool = True,
        seed: int = 0,
        drop_last: bool = False,
    ) -> None:
        (...)
  
    def __iter__(self) -> Iterator[_T_co]:        
        (...)
        indices = indices[self.rank : self.total_size : self.num_replicas]
        (...)
可提升方面
“This sampler is inherently lossy when used with torchft. torchft occasionally drops batches on rejoining and if a replica group is down that group examples will never be used. This can lead to imbalances if using a small dataset.”
- To fix, Sampler和Lighthouse要有沟通
DDP Wrapper
- TorchFT have not been extensively tested with Pytorch DDP
- Using TorchFT in DDP requires a few hacks (see link)
Transport
- Process Group based transport
- Uses the given process group to do send and receive operations
- Becomes self._checkpoint_transport in manager.py. 
- http has metadata whilst process-group based does not.

Uses store_prefixed_addr for pg configuration
Fault Tolerant Function Implementation
Running Illustrative Example
# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.
# fsdp_test.py

import multiprocessing
import os
import unittest
from concurrent.futures import ProcessPoolExecutor
from typing import Any, Dict, Tuple
from unittest.mock import Mock

import torch
import torch.distributed as dist
from torch import nn
from torch._C._distributed_c10d import (
    AllgatherOptions,
    AllreduceOptions,
    BroadcastOptions,
    ReduceOp,
    _resolve_process_group,
)
from torch.distributed import (
    ReduceOp,
    TCPStore,
    Work,
    _functional_collectives,
    get_world_size,
)
from torch.distributed._composable.fsdp import fully_shard
from torch.distributed.device_mesh import init_device_mesh

from torchft.manager import Manager
from torchft.process_group import ManagedProcessGroup, ft_init_device_mesh


class FSDPTest(unittest.TestCase):
    @staticmethod
    def _test_fsdp(world_size: int, rank: int) -> None:
        torch.cuda.set_device(rank)

        group_size = world_size // 2
        group = rank // group_size
        group_rank = rank % group_size

        os.environ["MASTER_ADDR"] = "127.0.0.1"
        os.environ["MASTER_PORT"] = str(12346 + group)
        os.environ["RANK"] = str(group_rank)
        os.environ["WORLD_SIZE"] = str(group_size)

        manager = Mock(spec=Manager)
        device_mesh = ft_init_device_mesh(
            device_type="cuda",
            mesh_shape=(2, 2),
            mesh_dim_names=("dp_replicate", "dp_shard"),
            replicate_dim=0,
            manager=manager,
        )
        manager.num_participants.return_value = 1
        model = nn.Linear(128, 128).cuda()
        batch = torch.randn(4, 128).cuda()
        shard_model = fully_shard(model, mesh=device_mesh)
        shard_model(batch).mean().backward()

    # pyre-ignore[56]: Pyre was not able to infer the type of argument
    @unittest.skipIf(torch.cuda.device_count() < 4, "Not enough GPUs")
    def test_fsdp(self) -> None:
        context = multiprocessing.get_context("spawn")
        with ProcessPoolExecutor(max_workers=4, mp_context=context) as executor:
            futures = []
            for i in range(4):
                future = executor.submit(self._test_fsdp, 4, i)
                futures.append(future)
Process Group Initialization (Warren)
1. Device Mesh
2. Process Group
Recovery (Danny)

Appendix
Torch.Distributed Internals
https://pytorch.org/tutorials/intermediate/process_group_cpp_extension_tutorial.html
https://medium.com/@eeyuhao/pytorch-distributed-a-bottom-up-perspective-e3159ee2c2e7

这个看懂以后他现在自己在pytorch distributed上面自己custom的逻辑其实就是他的这个checkpoint。我觉得看懂以后借鉴意义会挺大的，我们之后自己做其他checkpointing的方式按照这个来应该也不会难。
然后他做process group这个其实主要是在pytorch distributed正常的process group上面多了一些error handling。主要逻辑就是每一个error都会直接到manager那个层面选择怎么handle，不会在下面直接报错。manager发现问题以后就会和lighthouse汇报，然后自己看怎么样解决问题。这个我还在看。
Pytorch Process Group可以理解为一个CCL函数 (e.g. all-reduce) participate的machine，在process group object上可以call all reduce这些。process group可以手动建造，也可以用DeviceMesh https://pytorch.org/tutorials/recipes/distributed_device_mesh.html （会方便一些）。
每一个process group建造的时候会有一个rendezvous的过程 https://pytorch.org/docs/stable/elastic/rendezvous.html，要用tcp store来做我们昨天说的强一致性，做world size和rank的synchronization。这个每一次machine加入或者退出都需要重新做，但是这个应该是torchelastic里面就已经有了。
Work
Work is a crucial abstraction in torch.distributed. It enables asynchronous operations for the collective. 



Torchelastic and TorchFT
关于这个TorchFT和torchelastic的关系其实我也没有完全弄清楚。因为torchft用的是torchrun这个launcher，应该是直接integrate进去torchelastic的功能的https://pytorch.org/docs/stable/elastic/run.html。你看一下我理解的对不对。对process group implementation理解我觉得我自己差的最多的是torchft和torchelastic分别到底handle了什么样的错误。
