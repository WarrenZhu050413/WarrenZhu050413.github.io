@inproceedings{AlibabaHPN,
author = {Qian, Kun and Xi, Yongqing and Cao, Jiamin and Gao, Jiaqi and Xu, Yichi and Guan, Yu and Fu, Binzhang and Shi, Xuemei and Zhu, Fangbo and Miao, Rui and Wang, Chao and Wang, Peng and Zhang, Pengcheng and Zeng, Xianlong and Ruan, Eddie and Yao, Zhiping and Zhai, Ennan and Cai, Dennis},
title = {Alibaba HPN: A Data Center Network for Large Language Model Training},
year = {2024},
isbn = {9798400706141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651890.3672265},
doi = {10.1145/3651890.3672265},
abstract = {This paper presents HPN, Alibaba Cloud's data center network for large language model (LLM) training. Due to the differences between LLMs and general cloud computing (e.g., in terms of traffic patterns and fault tolerance), traditional data center networks are not well-suited for LLM training. LLM training produces a small number of periodic, bursty flows (e.g., 400Gbps) on each host. This characteristic of LLM training predisposes Equal-Cost Multi-Path (ECMP) to hash polarization, causing issues such as uneven traffic distribution. HPN introduces a 2-tier, dual-plane architecture capable of interconnecting 15K GPUs within one Pod, typically accommodated by the traditional 3-tier Clos architecture. Such a new architecture design not only avoids hash polarization but also greatly reduces the search space for path selection. Another challenge in LLM training is that its requirement for GPUs to complete iterations in synchronization makes it more sensitive to singlepoint failure (typically occurring on ToR). HPN proposes a new dual-ToR design to replace the single-ToR in traditional data center networks. HPN has been deployed in our production for more than eight months. We share our experience in designing, and building HPN, as well as the operational lessons of HPN in production.},
booktitle = {Proceedings of the ACM SIGCOMM 2024 Conference},
pages = {691â€“706},
numpages = {16},
keywords = {network architecture, AI infrastructure, large language model, model training, data center networks},
location = {Sydney, NSW, Australia},
series = {ACM SIGCOMM '24}
}


@techreport{NVIDIADGXSuperPOD,
  author = {NVIDIA},
  title = {DGX SuperPOD Reference Architecture for DGX H100 Systems},
  year = {2023},
  institution = {NVIDIA},
  url = {https://docs.nvidia.com/dgx-superpod-reference-architecture-dgx-h100.pdf},
  note = {Accessed: 2023-10-01}
}

@misc{C4dong2024,
      title={Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach}, 
      author={Jianbo Dong and Bin Luo and Jun Zhang and Pengcheng Zhang and Fei Feng and Yikai Zhu and Ang Liu and Zian Chen and Yi Shi and Hairong Jiao and Gang Lu and Yu Guan and Ennan Zhai and Wencong Xiao and Hanyu Zhao and Man Yuan and Siran Yang and Xiang Li and Jiamang Wang and Rui Men and Jianwei Zhang and Huang Zhong and Dennis Cai and Yuan Xie and Binzhang Fu},
      year={2024},
      eprint={2406.04594},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2406.04594}, 
}